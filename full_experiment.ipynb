{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import skimage.io\n",
    "import skimage.segmentation\n",
    "import skimage.morphology\n",
    "\n",
    "import sys\n",
    "__file__ = 'full_experiment.ipynb'\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import utils.dirtools  # utils package should has __init__.py in it\n",
    "import utils.augmentation\n",
    "import utils.model_builder\n",
    "import utils.data_provider\n",
    "import utils.metrics\n",
    "import utils.objectives\n",
    "import utils.evaluation\n",
    "\n",
    "import keras.backend\n",
    "import keras.callbacks\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import config_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build session running on GPU 1\n",
    "configuration = tf.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "# configuration.gpu_options.visible_device_list = \"0, 1\"\n",
    "session = tf.Session(config = configuration)\n",
    "\n",
    "# apply session\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vars[\"root_directory\"] = 'DATA/DNA_FISH/'\n",
    "experiment_name = '05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- experiment 03: train on FISH, \n",
    "- experiment 04: continue train on DNA_FISH  \n",
    "- experiment 05: directly train on DNA_FISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vars = utils.dirtools.setup_working_directories(config_vars)\n",
    "config_vars = utils.dirtools.setup_experiment(config_vars, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "file_list = os.listdir(config_vars[\"raw_images_dir\"])\n",
    "img = skimage.io.imread(config_vars[\"raw_images_dir\"] + file_list[-1])\n",
    "\n",
    "figure, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[1].hist(img.flatten(), bins=100)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(config_vars[\"normalized_images_dir\"], exist_ok=True)\n",
    "os.makedirs(config_vars[\"boundary_labels_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# normalize images\n",
    "\n",
    "if config_vars[\"transform_images_to_PNG\"]:\n",
    "    filelist = sorted(os.listdir(config_vars[\"raw_images_dir\"]))\n",
    "    # run over all raw images\n",
    "    for filename in tqdm(filelist):\n",
    "        # load image and its annotation\n",
    "        orig_img = skimage.io.imread(config_vars[\"raw_images_dir\"] + filename)       \n",
    "        # normalize to [0,1]\n",
    "        percentile = 99.9\n",
    "        high = np.percentile(orig_img, percentile)  # maximum for all image\n",
    "        low = np.percentile(orig_img, 100-percentile)\n",
    "\n",
    "        img = np.minimum(high, orig_img)\n",
    "        img = np.maximum(low, img)\n",
    "\n",
    "        # gives float64, thus cast to 8 bit later\n",
    "        img = (img - low) / (high - low) \n",
    "        img = skimage.img_as_ubyte(img) \n",
    "\n",
    "        skimage.io.imsave(config_vars[\"normalized_images_dir\"] + filename[:-3] + 'png', img)    \n",
    "else:\n",
    "    config_vars[\"normalized_images_dir\"] = config_vars[\"raw_images_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# segmentation to boundary\n",
    "\n",
    "filelist = sorted(os.listdir(config_vars[\"raw_annotations_dir\"]))\n",
    "filelist = [x for x in filelist if x.endswith('png')]\n",
    "total_objects = 0\n",
    "\n",
    "# run over all raw images\n",
    "for filename in tqdm(filelist):\n",
    "    # GET ANNOTATION\n",
    "    annot = skimage.io.imread(config_vars[\"raw_annotations_dir\"] + filename)\n",
    "    \n",
    "    # label the annotations nicely to prepare for future filtering operation\n",
    "    annot = skimage.morphology.label(annot)\n",
    "    total_objects += len(np.unique(annot)) - 1\n",
    "    \n",
    "    # filter small objects, e.g. micronulcei\n",
    "    annot = skimage.morphology.remove_small_objects(annot, min_size=config_vars[\"min_nucleus_size\"])\n",
    "    # find boundaries\n",
    "    boundaries = skimage.segmentation.find_boundaries(annot)\n",
    "    # make the boundary wider\n",
    "    for k in range(2, config_vars[\"boundary_size\"], 2):\n",
    "        boundaries = skimage.morphology.binary_dilation(boundaries)\n",
    "        \n",
    "    # 3-CHANNEL BINARY LABEL\n",
    "    # prepare buffer for binary label\n",
    "    label_binary = np.zeros((annot.shape + (3,)))  # cannot use dtype=np.uint8 here\n",
    "    # write binary label\n",
    "    label_binary[(annot == 0) & (boundaries == 0), 0] = 1  # bg\n",
    "    label_binary[(annot != 0) & (boundaries == 0), 1] = 1  # cell\n",
    "    label_binary[boundaries == 1, 2] = 1  # boundary\n",
    "    # Convert an image to unsigned byte format, with values in [0, 255]\n",
    "    label_binary = skimage.img_as_ubyte(label_binary)\n",
    "    # save it - converts image to range from 0 to 255\n",
    "    skimage.io.imsave(config_vars[\"boundary_labels_dir\"] + filename, label_binary)\n",
    "    \n",
    "print(\"Total objects: \",total_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(config_vars[\"normalized_images_dir\"])\n",
    "image_list = [x for x in file_list if x.endswith(\"png\")]\n",
    "len(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up train-valid split EVERY-TIME\n",
    "def create_image_lists(dir_raw_images):\n",
    "    file_list = os.listdir(dir_raw_images)\n",
    "    image_list = [x for x in file_list if x.endswith(\"png\")]\n",
    "    image_list = sorted(image_list)\n",
    "\n",
    "    image_list_train_aug = []\n",
    "    image_list_test = []\n",
    "    image_list_validation = image_list[63:]\n",
    "\n",
    "    image_list_2 = image_list[:63]\n",
    "    random.shuffle(image_list_2)\n",
    "    image_list_train = image_list_2\n",
    "    \n",
    "#     image_list_train = []\n",
    "#     image_list_validation = image_list  \n",
    "\n",
    "    return image_list_train, image_list_test, image_list_validation, image_list_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list_training, list_test, list_validation, list_training_aug] = create_image_lists(\n",
    "    config_vars[\"normalized_images_dir\"],\n",
    "#         config_vars[\"training_fraction\"],\n",
    "#         config_vars[\"validation_fraction\"]\n",
    ")\n",
    "\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_training\"], list_training)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_test\"], list_test)\n",
    "utils.dirtools.write_path_files(config_vars[\"path_files_validation\"], list_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''use matlab-labelled data to augment''' \n",
    "config_vars[\"path_files_training_aug\"] = 'FISH/training_aug.txt'\n",
    "\n",
    "# modify the write path method to add 'raw_masks/' ahead of the name\n",
    "utils.dirtools.write_path_files2(config_vars[\"path_files_training_aug\"], list_training_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_partitions = utils.dirtools.read_data_partitions(config_vars, load_augmented=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append aug filename to training split\n",
    "with open(config_vars[\"path_files_training_aug\"]) as f:\n",
    "    data_partitions[\"training\"] += f.read().splitlines()\n",
    "    \n",
    "data_partitions[\"training\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# setup data-generator\n",
    "train_gen = utils.data_provider.random_sample_generator(\n",
    "    config_vars[\"normalized_images_dir\"],\n",
    "    config_vars[\"boundary_labels_dir\"],\n",
    "    data_partitions[\"training\"],\n",
    "    config_vars[\"batch_size\"],\n",
    "    config_vars[\"pixel_depth\"],\n",
    "    config_vars[\"crop_size\"],\n",
    "    config_vars[\"crop_size\"],\n",
    "    config_vars[\"rescale_labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "val_gen = utils.data_provider.single_data_from_images(\n",
    "     config_vars[\"normalized_images_dir\"],\n",
    "     config_vars[\"boundary_labels_dir\"],\n",
    "     data_partitions[\"validation\"],\n",
    "     config_vars[\"val_batch_size\"],\n",
    "     config_vars[\"pixel_depth\"],\n",
    "     config_vars[\"crop_size\"],\n",
    "     config_vars[\"crop_size\"],\n",
    "     config_vars[\"rescale_labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = utils.model_builder.get_model_3_class(config_vars[\"crop_size\"], \n",
    "                                              config_vars[\"crop_size\"], activation=None)\n",
    "# model.summary()\n",
    "\n",
    "#loss = \"categorical_crossentropy\"\n",
    "loss = utils.objectives.weighted_crossentropy\n",
    "\n",
    "metrics = [keras.metrics.categorical_accuracy, \n",
    "           utils.metrics.channel_recall(channel=0, name=\"background_recall\"), \n",
    "           utils.metrics.channel_precision(channel=0, name=\"background_precision\"),\n",
    "           utils.metrics.channel_recall(channel=1, name=\"interior_recall\"), \n",
    "           utils.metrics.channel_precision(channel=1, name=\"interior_precision\"),\n",
    "           utils.metrics.channel_recall(channel=2, name=\"boundary_recall\"), \n",
    "           utils.metrics.channel_precision(channel=2, name=\"boundary_precision\"),\n",
    "          ]\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=config_vars[\"learning_rate\"])\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "log_folder = 'logs/'\n",
    "csv = keras.callbacks.CSVLogger(filename=config_vars[\"csv_log_file\"])\n",
    "tboard = keras.callbacks.TensorBoard(log_dir=log_folder + experiment_name, \n",
    "                                      histogram_freq=0, \n",
    "                                      batch_size=32, \n",
    "                                      write_graph=True, \n",
    "                                      write_grads=False, write_images=True,\n",
    "                                      update_freq='epoch')\n",
    "# add ModelCheckpoints\n",
    "# monitor val-loss\n",
    "weights_filename = log_folder + experiment_name + '/model-{epoch:02d}-{val_loss:.2f}.h5'\n",
    "modelckp = keras.callbacks.ModelCheckpoint(weights_filename, verbose=1, period=1,\n",
    "                                     save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# min_delta: threshold for measuring the new optimum,\n",
    "#       to only focus on significant changes.\n",
    "# cooldown: number of epochs to wait before resuming\n",
    "#       normal operation after lr has been reduced.\n",
    "reducelr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, \n",
    "                                             verbose=1, mode='min', min_lr=1e-7, \n",
    "                                             cooldown=10, min_delta=1e-4)\n",
    "# min_lr could be smaller\n",
    "\n",
    "callbacks = [csv, tboard, modelckp, reducelr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "weight_h5_file = 'DATA/FISH/experiments/03/model.hdf5'\n",
    "if os.path.isfile(weight_h5_file):\n",
    "    try:\n",
    "        model.load_weights(weight_h5_file)\n",
    "    except:\n",
    "        print('the model {} can not  be loaded'.format(weight_h5_file))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vars[\"epochs\"] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "statistics = model.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=config_vars[\"steps_per_epoch\"],\n",
    "    epochs=config_vars[\"epochs\"],\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=int(len(data_partitions[\"validation\"])/config_vars[\"val_batch_size\"]),\n",
    "    callbacks=callbacks,\n",
    "    verbose = 1\n",
    ")\n",
    "print('Done! :)')\n",
    "\n",
    "# save one weight at the end of the training\n",
    "model.save_weights(config_vars[\"model_file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vars['model_file'] = 'DATA/FISH/experiments/03/model.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = \"validation\"\n",
    "image_names = [os.path.join(config_vars[\"normalized_images_dir\"], f) \\\n",
    "               for f in data_partitions[partition]]\n",
    "\n",
    "imagebuffer = skimage.io.imread_collection(image_names)\n",
    "images = imagebuffer.concatenate()\n",
    "\n",
    "dim1 = images.shape[1]\n",
    "dim2 = images.shape[2]\n",
    "images = images.reshape((-1, dim1, dim2, 1))\n",
    "# preprocess (assuming images are encoded as 8-bits in the preprocessing step)\n",
    "images = images / 255\n",
    "\n",
    "# build model and load weights\n",
    "model = utils.model_builder.get_model_3_class(dim1, dim2)\n",
    "model.load_weights(config_vars[\"model_file\"])\n",
    "\n",
    "# Normal prediction time\n",
    "predictions = model.predict(images, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def empty_dir(folder):\n",
    "    print('empty directory: ', folder)\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "empty_dir(config_vars[\"probmap_out_dir\"])\n",
    "empty_dir(config_vars[\"labels_out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary to segmentation\n",
    "\n",
    "for i in range(len(images)):\n",
    "    filename = imagebuffer.files[i]\n",
    "    filename = os.path.basename(filename)\n",
    "    print(filename)\n",
    "    \n",
    "    probmap = predictions[i].squeeze()\n",
    "#     skimage.io.imsave(config_vars[\"probmap_out_dir\"] + filename, probmap)\n",
    "    \n",
    "    pred = utils.metrics.probmap_to_pred(probmap, config_vars[\"boundary_boost_factor\"])\n",
    "    label = utils.metrics.pred_to_label(pred, config_vars[\"cell_min_size\"])\n",
    "#     skimage.io.imsave(config_vars[\"labels_out_dir\"] + filename, label)\n",
    "    \n",
    "    if (i < 3):\n",
    "        plt.imshow(probmap)\n",
    "        plt.show()\n",
    "        plt.imshow(pred)\n",
    "        plt.show()\n",
    "        plt.imshow(label)\n",
    "        plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Display prediction along with ground truth to visualize errors\n",
    "\n",
    "def show(ground_truth, prediction, threshold=0.5, image_name=\"N\"):\n",
    "    # Compute Intersection over Union\n",
    "    IOU = utils.evaluation.intersection_over_union(ground_truth, prediction)\n",
    "\n",
    "    # Create diff map\n",
    "    diff = np.zeros(ground_truth.shape + (3,))  # become 3 channels\n",
    "    A = ground_truth.copy()\n",
    "    B = prediction.copy()\n",
    "    A[A > 0] = 1\n",
    "    B[B > 0] = 1\n",
    "    D = A - B\n",
    "    \n",
    "    # Object-level errors\n",
    "    C = IOU.copy()\n",
    "    C[C >= threshold] = 1\n",
    "    C[C < threshold] = 0\n",
    "    missed = np.where(np.sum(C, axis=1) == 0)[0]\n",
    "    extra = np.where(np.sum(C, axis=0) == 0)[0]\n",
    "\n",
    "    for m in missed:\n",
    "        diff[ground_truth == m+1, 0] = 1\n",
    "    for e in extra:\n",
    "        diff[prediction == e+1, 2] = 1\n",
    "    \n",
    "    # Display figures\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(18,6))\n",
    "    ax[0].imshow(ground_truth)\n",
    "    ax[0].set_title(\"True objects: \" + str(len(np.unique(ground_truth))))\n",
    "    ax[1].imshow(diff)\n",
    "    ax[1].set_title(\"Segmentation errors: \" + str(len(missed)))\n",
    "    ax[2].imshow(prediction)\n",
    "    ax[2].set_title(\"Predicted objects:\" + str(len(np.unique(prediction))))\n",
    "    ax[3].imshow(IOU)\n",
    "    ax[3].set_title(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_images = data_partitions[partition]  # validation\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Image\", \"Threshold\", \"F1\", \"Jaccard\", \"TP\", \"FP\", \"FN\"])\n",
    "false_negatives = pd.DataFrame(columns=[\"False_Negative\", \"Area\"])\n",
    "splits_merges = pd.DataFrame(columns=[\"Image_Name\", \"Merges\", \"Splits\"])\n",
    "\n",
    "for image_name in all_images:\n",
    "    # Load ground truth data\n",
    "    img_filename = os.path.join(config_vars[\"raw_annotations_dir\"], image_name)\n",
    "    ground_truth = skimage.io.imread(img_filename)\n",
    "    if len(ground_truth.shape) == 3:\n",
    "        ground_truth = ground_truth[:,:,0]\n",
    "    \n",
    "    # Transform to label matrix\n",
    "    ground_truth = skimage.morphology.label(ground_truth)\n",
    "    \n",
    "    # Load predictions\n",
    "    pred_filename = os.path.join(config_vars[\"labels_out_dir\"], image_name)\n",
    "    prediction = skimage.io.imread(pred_filename)\n",
    "    \n",
    "    # Apply object dilation\n",
    "    if config_vars[\"object_dilation\"] > 0:\n",
    "        struct = skimage.morphology.square(config_vars[\"object_dilation\"])\n",
    "        prediction = skimage.morphology.dilation(prediction, struct)\n",
    "    elif config_vars[\"object_dilation\"] < 0:\n",
    "        struct = skimage.morphology.square(-config_vars[\"object_dilation\"])\n",
    "        prediction = skimage.morphology.erosion(prediction, struct)\n",
    "        \n",
    "    # Relabel objects (cut margin of 30 pixels to make a fair comparison with DeepCell)\n",
    "    ground_truth = skimage.segmentation.relabel_sequential(ground_truth)[0] #[30:-30,30:-30])[0]\n",
    "    prediction = skimage.segmentation.relabel_sequential(prediction)[0] #[30:-30,30:-30])[0]\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    # Add result into pd dataframe one by one\n",
    "    results = utils.evaluation.compute_af1_results(\n",
    "        ground_truth, \n",
    "        prediction, \n",
    "        results, \n",
    "        image_name\n",
    "    )\n",
    "    \n",
    "    false_negatives = utils.evaluation.get_false_negatives(\n",
    "        ground_truth, \n",
    "        prediction, \n",
    "        false_negatives, \n",
    "        image_name\n",
    "    )\n",
    "    \n",
    "    splits_merges = utils.evaluation.get_splits_and_merges(\n",
    "        ground_truth, \n",
    "        prediction, \n",
    "        splits_merges, \n",
    "        image_name\n",
    "    )\n",
    "    \n",
    "    # Display an example image\n",
    "    if image_name == all_images[0]:\n",
    "        show(ground_truth, prediction, image_name=image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_performance = results.groupby(\"Threshold\").mean().reset_index()\n",
    "average_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = results.groupby(\"Image\").mean().reset_index()\n",
    "R.sort_values(by=\"F1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sb.jointplot(data=R[R[\"F1\"] > 0.4], x=\"Jaccard\", y=\"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy results\n",
    "\n",
    "sb.regplot(data=average_performance, x=\"Threshold\", y=\"F1\", order=3, ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print Average F1\n",
    "\n",
    "average_F1_score = average_performance[\"F1\"].mean()\n",
    "jaccard_index = average_performance[\"Jaccard\"].mean()\n",
    "print(\"Average F1 score:\", average_F1_score)\n",
    "print(\"Jaccard index:\", jaccard_index)\n",
    "\n",
    "# Summarize False Negatives by area\n",
    "\n",
    "false_negatives = false_negatives[false_negatives[\"False_Negative\"] == 1]\n",
    "\n",
    "false_negatives.groupby(\n",
    "    pd.cut(\n",
    "        false_negatives[\"Area\"], \n",
    "        [0, 250, 625, 900, 10000], # Area intervals\n",
    "        labels = [\"Tiny nuclei\",\"Small nuclei\",\"Normal nuclei\",\"Large nuclei\"],\n",
    "    )\n",
    ")[\"False_Negative\"].sum()\n",
    "\n",
    "# Summarize splits and merges\n",
    "\n",
    "print(\"Splits:\", np.sum(splits_merges[\"Splits\"]))\n",
    "print(\"Merges:\", np.sum(splits_merges[\"Merges\"]))\n",
    "\n",
    "# Report false positives\n",
    "\n",
    "print(\"Extra objects (false postives): \", \n",
    "      results[results[\"Threshold\"].round(3) == 0.7].sum()[\"FP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINT Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dirs = ['DATA/DNA_FISH/norm_images/',\n",
    "              'DATA/IF_images/norm_images/',\n",
    "              'DATA/Lineage_Tracking_ZY/norm_images/',\n",
    "              'DATA/Lineage_Tracking_KZ/norm_images/']\n",
    "\n",
    "labels_dirs = ['DATA/DNA_FISH/experiments/04/out/segm/',\n",
    "              'DATA/IF_images/experiments/04/out/segm/',\n",
    "              'DATA/Lineage_Tracking_ZY/experiments/04/out/segm/',\n",
    "              'DATA/Lineage_Tracking_KZ/experiments/04/out/segm/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(raw_label, pred_label):\n",
    "    ground_truth = raw_label.copy()\n",
    "    prediction = pred_label.copy()\n",
    "    IOU = utils.evaluation.intersection_over_union(ground_truth, prediction)\n",
    "    diff = np.zeros(ground_truth.shape + (3,))  # become 3 channels\n",
    "    A = ground_truth.copy()\n",
    "    B = prediction.copy()\n",
    "    A[A > 0] = 1\n",
    "    B[B > 0] = 1\n",
    "    D = A - B\n",
    "    #diff[D > 0,:2] = 1\n",
    "    #diff[D < 0,1:] = 1\n",
    "    \n",
    "    # Object-level errors\n",
    "    C = IOU.copy()\n",
    "    threshold = 0.5  # if set to 0.8, more misses will appear, but 0.5 no miss\n",
    "    C[C >= threshold] = 1\n",
    "    C[C < threshold] = 0\n",
    "    missed = np.where(np.sum(C, axis = 1) == 0)[0]  # for original cell, none predict cell match \n",
    "    extra = np.where(np.sum(C, axis = 0) == 0)[0]  # for predict cell, none original cell match\n",
    "\n",
    "    for m in missed:\n",
    "        diff[ground_truth == m + 1, 0] = 1\n",
    "    for e in extra:\n",
    "        diff[prediction == e + 1, 2] = 1\n",
    "        \n",
    "    return diff, str(len(missed)), str(len(extra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(img_name):\n",
    "    original_images = []\n",
    "    pred_labels = []\n",
    "    for i in range(4):\n",
    "        ori_img_filename = os.path.join(images_dirs[i], img_name)\n",
    "        original_image = skimage.io.imread(ori_img_filename)\n",
    "    \n",
    "        pred_label_filename = os.path.join(labels_dirs[i], img_name)\n",
    "        pred_label = skimage.io.imread(pred_label_filename)\n",
    "        \n",
    "        struct = skimage.morphology.square(3)\n",
    "        pred_label = skimage.morphology.dilation(pred_label, struct)\n",
    "        pred_label = skimage.segmentation.relabel_sequential(pred_label)[0] #[30:-30,30:-30])[0]\n",
    "            \n",
    "        # make graph easier to look\n",
    "        inc = lambda x: x if x == 0 else x + 100\n",
    "        inc = np.vectorize(inc)\n",
    "        pred_label = inc(pred_label)\n",
    "        \n",
    "        original_images.append(original_image)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "    fig, ax = plt.subplots(2, 4, figsize=(20,10))\n",
    "    fig.suptitle(img_name)\n",
    "    ax[0][0].set_title(\"DNA_FISH\")\n",
    "    ax[0][0].imshow(original_images[0])\n",
    "    ax[0][1].set_title(\"IF_images\")\n",
    "    ax[0][1].imshow(original_images[1])\n",
    "    ax[0][2].set_title(\"Linear_Tracking_zy\")\n",
    "    ax[0][2].imshow(original_images[2])\n",
    "    ax[0][3].set_title(\"Linear_Tracking_kz\")\n",
    "    ax[0][3].imshow(original_images[3])\n",
    "    \n",
    "#     ax[1][0].set_title(\"DNA_FISH\")\n",
    "    ax[1][0].imshow(pred_labels[0])\n",
    "#     ax[1][1].set_title(\"IF_images\")\n",
    "    ax[1][1].imshow(pred_labels[1])\n",
    "#     ax[1][2].set_title(\"Linear_Tracking_zy\")\n",
    "    ax[1][2].imshow(pred_labels[2])\n",
    "#     ax[1][3].set_title(\"Linear_Tracking_kz\")\n",
    "    ax[1][3].imshow(pred_labels[3])\n",
    "\n",
    "\n",
    "    # plt.figure(figsize=(6,6))\n",
    "    # plt.imshow(original_image)  #, cmap=\"nipy_spectral\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.randint(74, size = 20):\n",
    "    img = \"{:04}\".format(i) + \".png\"\n",
    "    compare(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
